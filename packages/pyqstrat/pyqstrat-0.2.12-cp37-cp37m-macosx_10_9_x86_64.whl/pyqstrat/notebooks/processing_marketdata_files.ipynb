{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Marketdata Files\n",
    "\n",
    "This notebook shows an example of processing a set of market data files into trades, quotes and open interest records using multiple processes to utilize all the cores on your machine.\n",
    "\n",
    "It uses the marketdata_processor and pyqstrat_cpp submodules of pyqstrat.  pyqstrat_cpp contains functionality written in C++ since I use this for processing large tick data files, which can be hundreds of gigabytes and would take too long to process in Python.\n",
    "\n",
    "Here are the stages we will go through.\n",
    "\n",
    "1.  File discovery : Generating the list of files we need to process\n",
    "2.  Decompressing and Reading Files : pyqstrat can handle gzip, bz2 and xz files\n",
    "3.  Filtering: Discard any records we don't care about or that are clearly in error\n",
    "3.  Parsing : Parsing lines from the file into quotes, trades or open interest records\n",
    "4.  Cleaning : For example, some vendors use 0 to indicate there was no bid, we can replace it with NAN for easier downstream processing.\n",
    "5.  Aggregation : Aggregate quotes and trades in various ways, for example, into 1 minute bars or into instantaneous top of book records and write these to disk.\n",
    "6.  Writing: Finally we write the aggregated data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T22:02:34.607070Z",
     "start_time": "2018-10-15T22:02:34.563021Z"
    }
   },
   "source": [
    "## Test Data Creation\n",
    "\n",
    "First, lets create some data.  We will create a few files in the temp directory representing data that a market data vendor would have sent us.  This data is options data formatted like Algoseek's options data files but the actual prices and quantities are not real.  We will create a gzipped file with some open interest, quote and trade records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.139Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pyqstrat as pq\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "open_interest_records = \\\n",
    "b'''Timestamp,Ticker,Type,Side,Info,PutCall,Expiration,Strike,Quantity,Premium,Exchange\n",
    "06:30:31.461,SPXW,O, , ,C,20160316,21000000,1029,0,W\n",
    "06:30:31.461,SPXW,O, , ,P,20160316,20700000,5,0,W\n",
    "06:30:31.461,SPXW,O, , ,C,20160316,19750000,205,0,W\n",
    "06:30:31.461,SPXW,O, , ,C,20160316,19600000,320,0,W\n",
    "06:30:31.461,SPXW,O, , ,C,20160316,19450000,256,0,W\n",
    "06:30:31.461,SPXW,O, , ,C,20160316,19300000,9,0,W\n",
    "06:30:31.461,SPXW,O, , ,C,20160316,19150000,3,0,W\n",
    "06:30:31.461,SPXW,O, , ,C,20160316,19000000,253,0,W\n",
    "06:30:31.461,SPXW,O, , ,C,20160316,18850000,5,0,W\n",
    "06:30:31.462,SPXW,O, , ,P,20160316,18650000,194,0,W\n",
    "06:30:31.463,SPXW,O, , ,P,20160316,18350000,722,0,W\n",
    "06:30:31.463,SPXW,O, , ,P,20160316,18200000,204,0,W\n",
    "06:30:31.463,SPXW,O, , ,C,20160316,20450000,690,0,W\n",
    "06:30:31.463,SPXW,O, , ,P,20160316,20250000,498,0,W\n",
    "06:30:31.463,SPXW,O, , ,C,20160316,20050000,559,0,W\n",
    "06:30:31.464,SPXW,O, , ,C,20160316,19900000,530,0,W\n",
    "'''\n",
    "\n",
    "quote_records = \\\n",
    "b'''10:19:12.950,BRKA,F,O, ,C,20160316,20450000,115,15500,W\n",
    "10:19:13.140,BRKA,F,O, ,C,20160316,20300000,85,49000,W\n",
    "10:19:13.140,BRKA,F,B, ,C,20160316,20300000,38,45000,W\n",
    "10:19:13.140,BRKA,F,O, ,C,20160316,20350000,87,34000,W\n",
    "10:19:13.140,BRKA,F,B, ,C,20160316,20350000,44,31000,W\n",
    "10:19:13.140,BRKA,F,O, ,C,20160316,20100000,48,143000,W\n",
    "10:19:13.140,BRKA,F,B, ,C,20160316,20100000,30,136000,W\n",
    "10:19:13.141,BRKA,F,O, ,P,20160316,20150000,32,120000,W\n",
    "10:19:13.141,BRKA,F,B, ,P,20160316,20150000,55,112000,W\n",
    "10:19:13.141,BRKA,F,O, ,P,20160316,20050000,51,78000,W\n",
    "10:19:13.141,BRKA,F,B, ,P,20160316,20050000,28,73000,W\n",
    "10:19:13.141,BRKA,F,O, ,P,20160316,20250000,14,175000,W\n",
    "10:19:13.141,BRKA,F,B, ,P,20160316,20250000,21,161000,W\n",
    "10:19:13.141,BRKA,F,O, ,P,20160316,20200000,15,145000,W\n",
    "10:19:13.141,BRKA,F,B, ,P,20160316,20200000,42,136000,W\n",
    "10:20:13.152,BRKA,F,O, ,C,20160316,20050000,24,179000,W\n",
    "10:20:13.152,BRKA,F,B, ,C,20160316,20450000,18,165000,W\n",
    "10:20:13.152,BRKA,F,O, ,C,20160316,20300000,85,49000,W\n",
    "10:20:13.152,BRKA,F,B, ,C,20160316,20300000,28,45000,W\n",
    "10:20:13.160,BRKA,F,O, ,C,20160316,20100000,52,143000,W\n",
    "10:20:13.160,BRKA,F,B, ,C,20160316,20100000,30,136000,W\n",
    "10:20:13.161,BRKA,F,O, ,C,20160316,20300000,85,49000,W\n",
    "10:20:13.161,BRKA,F,B, ,C,20160316,20300000,32,45000,W\n",
    "10:20:13.161,BRKA,F,O, ,P,20160316,20050000,55,78000,W\n",
    "10:20:13.161,BRKA,F,B, ,P,20160316,20050000,28,73000,W\n",
    "10:20:13.161,BRKA,F,O, ,P,20160316,20100000,20,96000,W\n",
    "10:20:13.161,BRKA,F,B, ,P,20160316,20100000,48,91000,W\n",
    "10:20:13.161,BRKA,F,O, ,P,20160316,20150000,36,120000,W\n",
    "10:20:13.161,BRKA,F,B, ,P,20160316,20150000,59,112000,W\n",
    "'''\n",
    "\n",
    "trade_records = \\\n",
    "b'''09:30:03.365,BRKA,T, , ,C,20160316,20200000,1,98000,W\n",
    "09:30:03.481,BRKA,T, , ,P,20160316,20650000,2,442000,W\n",
    "09:30:03.566,BRKA,T, ,L,P,20160316,20650000,1,6000,W\n",
    "09:30:03.568,BRKA,T, ,L,P,20160316,20650000,2,13500,W\n",
    "09:30:03.568,BRKA,T, ,L,P,20160316,19900000,1,34000,W\n",
    "09:30:04.473,BRKA,T, ,L,C,20160316,19450000,15,714000,W\n",
    "09:30:05.883,BRKA,T, ,L,C,20160316,20400000,4,24000,W\n",
    "09:30:05.884,BRKA,T, ,L,P,20160316,20100000,10,87500,W\n",
    "09:30:05.884,BRKA,T, ,L,P,20160316,20150000,10,109000,W\n",
    "09:30:05.886,BRKA,T, ,L,C,20160316,20150000,3,119000,W\n",
    "09:30:05.886,BRKA,T, ,L,C,20160316,20550000,3,7000,W\n",
    "09:30:05.886,BRKA,T, ,L,P,20160316,19750000,3,15000,W\n",
    "09:30:05.886,BRKA,T, ,L,P,20160316,20150000,3,110500,W\n",
    "09:31:09.285,BRKA,T, ,L,P,20160316,20650000,10,26000,W\n",
    "09:31:09.286,BRKA,T, ,L,P,20160316,20650000,10,33500,W\n",
    "09:31:11.491,BRKA,T, , ,P,20160316,20650000,24,28000,W\n",
    "09:31:11.586,BRKA,T, , ,P,20160316,19850000,12,28000,W\n",
    "09:31:12.805,BRKA,T, , ,P,20160316,19800000,34,22000,W\n",
    "09:31:12.831,BRKA,T, , ,P,20160316,19750000,44,17500,W\n",
    "09:31:12.863,BRKA,T, , ,P,20160316,19750000,13,17500,W\n",
    "09:31:13.640,BRKA,T, , ,P,20160316,19850000,1,28000,W\n",
    "09:31:18.232,BRKA,T, ,L,C,20160316,20350000,26,34000,W\n",
    "09:31:18.232,BRKA,T, ,L,C,20160316,20400000,26,23000,W\n",
    "09:31:19.176,BRKA,T, ,L,P,20160316,19200000,3,3000,W\n",
    "09:31:19.176,BRKA,T, ,L,P,20160316,19300000,3,3500,W\n",
    "09:31:21.639,BRKA,T, ,L,C,20160316,20350000,4,34000,W\n",
    "'''\n",
    "\n",
    "other_records = \\\n",
    "b'''09:30:03.566,BRKA, , ,Sample Info,P,20160316,19500000,,,W\n",
    "09:30:03.568,BRKA,X, ,Sample Info 2,P,20160316,19700000,,,W\n",
    "09:30:05.886,BRKA,X, ,Sample Info 3,C,20160316,20550000,,,W\n",
    "09:30:05.886,BRKA,X, ,Sample Info 4,P,20160316,19750000,,,W\n",
    "09:30:05.886,BRKA,X, ,Sample Info 5,P,20160316,20150000,,,W\n",
    "'''\n",
    "if os.path.isdir('/tmp'):\n",
    "    temp_dir = \"/tmp/\"\n",
    "else:\n",
    "    temp_dir =  tempfile.gettempdir()\n",
    "    \n",
    "input_filename = temp_dir + '/BRKA_2018-01-01_data'\n",
    "    \n",
    "from sys import platform\n",
    "if platform not in [\"win32\", \"cygwin\"]: # cannot read compressed files on windows yet because of link issues with boost\n",
    "    input_filename += '.gz'\n",
    "    import gzip\n",
    "    with gzip.open(input_filename, 'wb') as f:\n",
    "        f.write(open_interest_records + quote_records + trade_records + other_records)\n",
    "else:\n",
    "    with open(input_filename, 'w') as f:\n",
    "        f.write((open_interest_records + quote_records + trade_records + other_records).decode('utf-8'))\n",
    "    \n",
    "# Look at the data\n",
    "import pandas as pd\n",
    "pd.read_csv(input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T22:08:08.665014Z",
     "start_time": "2018-10-15T22:08:08.658946Z"
    }
   },
   "source": [
    "## File discovery\n",
    "\n",
    "We use the helper function object PathFileNameProvider to generate the list of files we need to process.  We then use the helper function object  \n",
    "SingleDirectoryFileNameMapper to figure out the output file name corresponding to each input file.  We will write output files with the same prefix\n",
    "as their corresponding input files but in a subdirectory called pyqstrat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.141Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "file_path = temp_dir + '/BRKA_*'\n",
    "\n",
    "if platform not in [\"win32\", \"cygwin\"]: # cannot read compressed files on windows yet because of link issues with boost\n",
    "    file_path += '.gz'\n",
    "\n",
    "input_filename_provider = pq.PathFileNameProvider(file_path)\n",
    "output_dir = temp_dir + '/pyqstrat'\n",
    "if not os.path.isdir(output_dir): os.mkdir(output_dir)\n",
    "output_file_prefix_mapper = pq.SingleDirectoryFileNameMapper(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "Lets first create the functions that let us know if a record is a trade, quote or open interest record.  We know the second field in the input file contains the type of record.  We will also create a function that will let us quickly discard any lines we don't care about before we start parsing them.  We can also use a record filter that will filter out trade records after they have been parsed for more fine grained filtering, but we don't need it here, so we let the record filter default to None.\n",
    "\n",
    "We implement IsOther differently than the other functions to show how we can write a function in Python and plug it into the processing instead of using the predefined C++ functions. This will be much slower than using C++ functions so avoid using frequently called Python functions in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.142Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "is_quote = pq.IsFieldInList(2, [\"F\", \"N\"])\n",
    "is_trade = pq.IsFieldInList(2, [\"T\"])\n",
    "is_open_interest = pq.IsFieldInList(2, [\"O\"])\n",
    "\n",
    "# Show how a function can be implemented in python instead of using the pyqstrat C++ predefined functions\n",
    "class PyIsFieldInList(pq.CheckFields):\n",
    "    # The following line is necessary due to the way pybind11 calls virtual functions.  \n",
    "    # See #https://pybind11.readthedocs.io/en/stable/advanced/classes.html\n",
    "    def __init__(self, field_idx, flags):\n",
    "        pq.CheckFields.__init__(self) \n",
    "        self.field_idx = field_idx\n",
    "        self.flags = flags\n",
    "        \n",
    "    def __call__(self, fields):\n",
    "        return fields[self.field_idx] in self.flags\n",
    "    \n",
    "is_other = PyIsFieldInList(2, [\"X\"])\n",
    "\n",
    "# Keep only the lines that contain one of these substrings.  We can also use pq.RegExLineFilter but that is much slower than checking for substrings\n",
    "line_filter = pq.SubStringLineFilter([\",T,\", \",F,\", \",N,\", \",O,\", \",X,\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "Lets now create functions that create parsers.  We are going to use the helper function objects TextQuoteParser, TextTradeParser, TextOpenInterestParser, TextOtherParser that create the corresponding types from a record in the file.  These are designed for compressed or uncompressed text files where price, qty, etc. are stored in delimited fields.  If that is not the case for your market data files, you will have to create your own parsers.  These function objects are written in C++ for performance.\n",
    "\n",
    "We use the helper function FastTimeMilliParser which parses timestamps that are in HH:MM:SS format (with 3 decimal places after the second) to store timestamps in milliseconds.  If your timestamps are in microseconds, you can use FastTimeMicroParser.  If they are in other formats you can use FormatTimestampParser which takes a C strftime date format as one of the inputs, but is slower.\n",
    "\n",
    "By default the system uses the helper function object PrintBadLineHandler for records that cannot be parsed.  This class prints out unparseable lines for debugging, or raises an Exception, depending on how its initialized.  You can implement your own handler for second chance, slower parsing of more complex records.\n",
    "\n",
    "By default, the sytem also uses the helper function price_qty_missing_data_handler to set prices and quantities that are 0 to NAN.  You can implement your own function object for other policies to deal with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.144Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "time_parser = pq.FixedWidthTimeParser(micros = False, hours_start = 0, hours_size = 2, minutes_start = 3, minutes_size = 2, seconds_start = 6, seconds_size = 2, \n",
    "                                     millis_start = 9, millis_size = 3)\n",
    "\n",
    "def create_quote_parser(base_date, headers):\n",
    "    timestamp_idx = headers.index('timestamp')\n",
    "    qty_idx = headers.index('quantity')\n",
    "    price_idx = headers.index('premium')\n",
    "    bid_offer_idx = headers.index('side')\n",
    "    # Id indices are used to uniquely identify an instrument.  Within this file, put/call, epxiry and strike uniquely identify an option\n",
    "    id_indices = pq.get_field_indices(['putcall', 'expiration', 'strike'], headers)\n",
    "    # Any other info besides qty, price, bid/offer and id that we want to store is stored in the meta field\n",
    "    meta_indices = pq.get_field_indices(['info', 'exchange'], headers)\n",
    "    # Prices in the input file are stored in thousands of cents so we divide them by 10000.0 to get dollars.\n",
    "    # Bids are stored as \"O\" and offers are stored as \"O\"\n",
    "    return pq.TextQuoteParser(is_quote, base_date, timestamp_idx, bid_offer_idx, price_idx, qty_idx, id_indices, meta_indices,\n",
    "                                    time_parser, \"B\", \"O\", 10000.0)\n",
    "                                 \n",
    "def create_trade_parser(base_date, headers):\n",
    "    timestamp_idx = headers.index('timestamp')\n",
    "    qty_idx = headers.index('quantity')\n",
    "    price_idx = headers.index('premium')\n",
    "    id_indices = pq.get_field_indices(['putcall', 'expiration', 'strike'], headers)\n",
    "    meta_indices = pq.get_field_indices(['info', 'exchange'], headers)\n",
    "    return pq.TextTradeParser(is_trade, base_date, timestamp_idx, price_idx, qty_idx, id_indices, meta_indices,\n",
    "                                    time_parser, 10000.0)\n",
    "                                 \n",
    "def create_open_interest_parser(base_date, headers):\n",
    "    timestamp_idx = headers.index('timestamp')\n",
    "    qty_idx = headers.index('quantity')\n",
    "    id_indices = pq.get_field_indices(['putcall', 'expiration', 'strike'], headers)\n",
    "    meta_indices = pq.get_field_indices(['info', 'exchange'], headers)\n",
    "    return pq.TextOpenInterestParser(is_open_interest, base_date, timestamp_idx, qty_idx, id_indices, meta_indices,\n",
    "                                    time_parser, 10000.0)\n",
    "                           \n",
    "def create_other_parser(base_date, headers):\n",
    "    timestamp_idx = headers.index('timestamp')\n",
    "    qty_idx = headers.index('quantity')\n",
    "    id_indices = pq.get_field_indices(['putcall', 'expiration', 'strike'], headers)\n",
    "    meta_indices = pq.get_field_indices(['info', 'exchange'], headers)\n",
    "    return pq.TextOtherParser(is_other, base_date, timestamp_idx, id_indices, meta_indices, time_parser)\n",
    "    \n",
    "def create_record_parser(base_date, headers):\n",
    "    \n",
    "    return pq.TextRecordParser([create_quote_parser(base_date, headers), \n",
    "                                create_trade_parser(base_date, headers), \n",
    "                                create_open_interest_parser(base_date, headers), \n",
    "                                create_other_parser(base_date, headers)], \n",
    "                               exclusive = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "We have a choice on how we want to aggregate these records.  We will use the helper class QuoteTOBAggregator which creates top-of-book quotes at a given frequency (or for all ticks if frequency is omitted) and TradeBarAggregator which creates trade bars at given freqency.  For Open Interest and OtherRecords, we keep all records.  You can use None for any of these functions if you only care about parsing Trades, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.145Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def create_aggregators(writer_creator, output_file_prefix, frequency = '1m'):\n",
    "    return [pq.QuoteTOBAggregator(writer_creator, output_file_prefix + \".tob\", frequency = frequency),\n",
    "            pq.TradeBarAggregator(writer_creator, output_file_prefix + \".trades\", frequency = frequency),\n",
    "            pq.AllOpenInterestAggregator(writer_creator, output_file_prefix + \".open_interest\"),\n",
    "            pq.AllOtherAggregator(writer_creator, output_file_prefix + \".other\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing\n",
    "\n",
    "By default pyqstrat writes files using Apache Arrow so we can quickly read them back in Python without having to do any type conversion, or imply datatypes.  You can change the writing format by setting the argument writer_creator to a function that creates your own class implementing the pyqstrat Writer interface.  \n",
    "\n",
    "Writers write data in batches.  This allows us to be able to read a set of records that fit in memory by reading a batch of records at a time.  The Trade and Quote Aggregators allow you to either create one batch per id or set a batch size.  Use the former when you are aggregating to a relatively small data set.  For example, if you are writing out 5 minute bars, chances are you can process a large input file without overwhelming memory.  In this case, the Arrow Writer also writes out a separate file that maps each instrument id to its corresponding batch.  \n",
    "\n",
    "If you need to record every tick, you might want to use the latter approach,  which will write out a batch to disk as soon as the aggregators have processed a certain number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing Up\n",
    "\n",
    "We are now ready to run the process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.147Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def process(input_filename):\n",
    "    pq.process_marketdata_file(input_filename,\n",
    "             output_file_prefix_mapper, \n",
    "             create_record_parser,\n",
    "             create_aggregators,\n",
    "             line_filter)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # arrow writer creates an empty *.done marker file to indicate when its finished processing an input file, delete it so we can rerun\n",
    "    done_file = output_dir + '/BRKA_2018-01-01_data.done'\n",
    "    if os.path.exists(done_file): os.remove(done_file)\n",
    "    with pq.ostream_redirect(stdout = True, stderr = True):\n",
    "        pq.process_marketdata(input_filename_provider, process, num_processes = 8)\n",
    "        \n",
    "def run_me():\n",
    "    # arrow writer creates an empty *.done marker file to indicate when its finished processing an input file, delete it so we can rerun\n",
    "    done_file = output_dir + '/BRKA_2018-01-01_data.done'\n",
    "    if os.path.exists(done_file): os.remove(done_file)\n",
    "    with pq.ostream_redirect(stdout = True, stderr = True):\n",
    "        pq.process_marketdata(input_filename_provider, process, num_processes = 8)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T23:21:21.040435Z",
     "start_time": "2018-10-15T23:21:21.035167Z"
    }
   },
   "source": [
    "We can now look at the output files using the Apache Arrow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.148Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.149Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pa.RecordBatchFileReader(pa.OSFile(output_dir + '/BRKA_2018-01-01_data.open_interest.arrow', 'r')).read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.151Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pa.RecordBatchFileReader(pa.OSFile(output_dir + '/BRKA_2018-01-01_data.trades.1m.arrow', 'r')).read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.152Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pa.RecordBatchFileReader(pa.OSFile(output_dir + '/BRKA_2018-01-01_data.tob.1m.arrow', 'r')).read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-05T04:39:07.154Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pa.RecordBatchFileReader(pa.OSFile(output_dir + '/BRKA_2018-01-01_data.other.arrow', 'r')).read_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
