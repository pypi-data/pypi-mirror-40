{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make OpenAI Gym like environment\n",
    "- This example uses DDPG(Deep Deterministic Policy Gradient) with pybullet_env\n",
    "- pybullet_env prerequisites: Open AI Gym, pybullet.\n",
    "\n",
    "pip install gym\n",
    "\n",
    "pip install pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: Environment '<class 'pybullet_envs.gym_pendulum_envs.InvertedPendulumBulletEnv'>' has deprecated methods. Compatibility code invoked.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import pybullet_envs\n",
    "import time\n",
    "env = gym.make(\"InvertedPendulumBulletEnv-v0\")\n",
    "env.render(mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space: Box(1,)\n",
      "action space high,low : [1.] [-1.]\n",
      "state space: Box(5,)\n",
      "state space high,low : [inf inf inf inf inf] [-inf -inf -inf -inf -inf]\n"
     ]
    }
   ],
   "source": [
    "print('action space:',env.action_space)\n",
    "print('action space high,low :',env.action_space.high,env.action_space.low)\n",
    "print('state space:',env.observation_space)\n",
    "print('state space high,low :',env.observation_space.high,env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import RL Algorithm\n",
    "\n",
    "Base agent needs core agent and an environment to interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlagent.agents import ExperienceReplayAgent\n",
    "from rlagent.algorithms import DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.shape\n",
    "ddpg = DDPG(state_shape, action_shape, tau=0.01, actor_lr=0.0001, critic_lr=0.001,\n",
    "            action_noise=True, add_memory=True)\n",
    "tf_agent = ExperienceReplayAgent(agent=ddpg, env=env, save_steps=10000, model_dir='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "actor (ActorFF)              (None, 1)                 9089      \n",
      "_________________________________________________________________\n",
      "critic (QCriticFF)           (None, 1)                 9153      \n",
      "_________________________________________________________________\n",
      "target_actor (ActorFF)       (None, 1)                 9089      \n",
      "_________________________________________________________________\n",
      "target_critic (QCriticFF)    (None, 1)                 9153      \n",
      "=================================================================\n",
      "Total params: 36,484\n",
      "Trainable params: 36,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf_agent.agent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running initial ops.\n",
      "INFO:tensorflow: Episode 1: total reward=23.0000, episode steps=23, trained steps=0\n",
      "INFO:tensorflow: Episode 2: total reward=20.0000, episode steps=20, trained steps=0\n",
      "INFO:tensorflow: Episode 3: total reward=20.0000, episode steps=20, trained steps=0\n",
      "INFO:tensorflow: Episode 4: total reward=25.0000, episode steps=25, trained steps=0\n",
      "INFO:tensorflow: Episode 5: total reward=20.0000, episode steps=20, trained steps=0\n",
      "INFO:tensorflow:Added 135 to ReplayBuffer. Starting training.\n",
      "INFO:tensorflow: Episode 6: total reward=27.0000, episode steps=27, trained steps=0\n",
      "INFO:tensorflow: Episode 7: total reward=21.0000, episode steps=21, trained steps=20\n",
      "INFO:tensorflow: Episode 8: total reward=25.0000, episode steps=25, trained steps=45\n",
      "INFO:tensorflow: Episode 9: total reward=28.0000, episode steps=28, trained steps=73\n",
      "INFO:tensorflow: Episode 10: total reward=25.0000, episode steps=25, trained steps=98\n",
      "INFO:tensorflow: Episode 11: total reward=26.0000, episode steps=26, trained steps=124\n",
      "INFO:tensorflow: Episode 12: total reward=22.0000, episode steps=22, trained steps=146\n",
      "INFO:tensorflow: Episode 13: total reward=24.0000, episode steps=24, trained steps=170\n",
      "INFO:tensorflow: Episode 14: total reward=13.0000, episode steps=13, trained steps=183\n",
      "INFO:tensorflow: Episode 15: total reward=10.0000, episode steps=10, trained steps=193\n",
      "INFO:tensorflow: Episode 16: total reward=10.0000, episode steps=10, trained steps=203\n",
      "INFO:tensorflow: Episode 17: total reward=11.0000, episode steps=11, trained steps=214\n",
      "INFO:tensorflow: Episode 18: total reward=13.0000, episode steps=13, trained steps=227\n",
      "INFO:tensorflow: Episode 19: total reward= 9.0000, episode steps=9, trained steps=236\n",
      "INFO:tensorflow: Episode 20: total reward=13.0000, episode steps=13, trained steps=249\n",
      "INFO:tensorflow: Episode 21: total reward=15.0000, episode steps=15, trained steps=264\n",
      "INFO:tensorflow: Episode 22: total reward= 9.0000, episode steps=9, trained steps=273\n",
      "INFO:tensorflow: Episode 23: total reward=10.0000, episode steps=10, trained steps=283\n",
      "INFO:tensorflow: Episode 24: total reward=12.0000, episode steps=12, trained steps=295\n",
      "INFO:tensorflow: Episode 25: total reward=10.0000, episode steps=10, trained steps=305\n",
      "INFO:tensorflow: Episode 26: total reward=13.0000, episode steps=13, trained steps=318\n",
      "INFO:tensorflow: Episode 27: total reward=22.0000, episode steps=22, trained steps=340\n",
      "INFO:tensorflow: Episode 28: total reward=33.0000, episode steps=33, trained steps=373\n",
      "INFO:tensorflow: Episode 29: total reward=13.0000, episode steps=13, trained steps=386\n",
      "INFO:tensorflow: Episode 30: total reward=26.0000, episode steps=26, trained steps=412\n",
      "INFO:tensorflow: Episode 31: total reward=33.0000, episode steps=33, trained steps=445\n",
      "INFO:tensorflow: Episode 32: total reward=14.0000, episode steps=14, trained steps=459\n",
      "INFO:tensorflow: Episode 33: total reward=16.0000, episode steps=16, trained steps=475\n",
      "INFO:tensorflow: Episode 34: total reward=17.0000, episode steps=17, trained steps=492\n",
      "INFO:tensorflow: Episode 35: total reward=86.0000, episode steps=86, trained steps=578\n",
      "INFO:tensorflow: Episode 36: total reward=49.0000, episode steps=49, trained steps=627\n",
      "INFO:tensorflow: Episode 37: total reward=35.0000, episode steps=35, trained steps=662\n",
      "INFO:tensorflow: Episode 38: total reward=35.0000, episode steps=35, trained steps=697\n",
      "INFO:tensorflow: Episode 39: total reward=19.0000, episode steps=19, trained steps=716\n",
      "INFO:tensorflow: Episode 40: total reward=41.0000, episode steps=41, trained steps=757\n",
      "INFO:tensorflow: Episode 41: total reward=30.0000, episode steps=30, trained steps=787\n",
      "INFO:tensorflow: Episode 42: total reward=26.0000, episode steps=26, trained steps=813\n",
      "INFO:tensorflow: Episode 43: total reward=30.0000, episode steps=30, trained steps=843\n",
      "INFO:tensorflow: Episode 44: total reward=39.0000, episode steps=39, trained steps=882\n",
      "INFO:tensorflow: Episode 45: total reward=32.0000, episode steps=32, trained steps=914\n",
      "INFO:tensorflow: Episode 46: total reward=25.0000, episode steps=25, trained steps=939\n",
      "INFO:tensorflow: Episode 47: total reward=24.0000, episode steps=24, trained steps=963\n",
      "INFO:tensorflow: Episode 48: total reward=40.0000, episode steps=40, trained steps=1003\n",
      "INFO:tensorflow: Episode 49: total reward=68.0000, episode steps=68, trained steps=1071\n",
      "INFO:tensorflow: Episode 50: total reward=41.0000, episode steps=41, trained steps=1112\n",
      "INFO:tensorflow: Episode 51: total reward=49.0000, episode steps=49, trained steps=1161\n",
      "INFO:tensorflow: Episode 52: total reward=42.0000, episode steps=42, trained steps=1203\n",
      "INFO:tensorflow: Episode 53: total reward=60.0000, episode steps=60, trained steps=1263\n",
      "INFO:tensorflow: Episode 54: total reward=55.0000, episode steps=55, trained steps=1318\n",
      "INFO:tensorflow: Episode 55: total reward=31.0000, episode steps=31, trained steps=1349\n",
      "INFO:tensorflow: Episode 56: total reward=62.0000, episode steps=62, trained steps=1411\n",
      "INFO:tensorflow: Episode 57: total reward=64.0000, episode steps=64, trained steps=1475\n",
      "INFO:tensorflow: Episode 58: total reward=50.0000, episode steps=50, trained steps=1525\n",
      "INFO:tensorflow: Episode 59: total reward=46.0000, episode steps=46, trained steps=1571\n",
      "INFO:tensorflow: Episode 60: total reward=131.0000, episode steps=131, trained steps=1702\n",
      "INFO:tensorflow: Episode 61: total reward=135.0000, episode steps=135, trained steps=1837\n",
      "INFO:tensorflow: Episode 62: total reward=29.0000, episode steps=29, trained steps=1866\n",
      "INFO:tensorflow: Episode 63: total reward=127.0000, episode steps=127, trained steps=1993\n",
      "INFO:tensorflow: Episode 64: total reward=81.0000, episode steps=81, trained steps=2074\n",
      "INFO:tensorflow: Episode 65: total reward=83.0000, episode steps=83, trained steps=2157\n",
      "INFO:tensorflow: Episode 66: total reward=87.0000, episode steps=87, trained steps=2244\n",
      "INFO:tensorflow: Episode 67: total reward=139.0000, episode steps=139, trained steps=2383\n",
      "INFO:tensorflow: Episode 68: total reward=83.0000, episode steps=83, trained steps=2466\n",
      "INFO:tensorflow: Episode 69: total reward=99.0000, episode steps=99, trained steps=2565\n",
      "INFO:tensorflow: Episode 70: total reward=108.0000, episode steps=108, trained steps=2673\n",
      "INFO:tensorflow: Episode 71: total reward=103.0000, episode steps=103, trained steps=2776\n",
      "INFO:tensorflow: Episode 72: total reward=118.0000, episode steps=118, trained steps=2894\n",
      "INFO:tensorflow: Episode 73: total reward=93.0000, episode steps=93, trained steps=2987\n",
      "INFO:tensorflow: Episode 74: total reward=112.0000, episode steps=112, trained steps=3099\n",
      "INFO:tensorflow: Episode 75: total reward=108.0000, episode steps=108, trained steps=3207\n",
      "INFO:tensorflow: Episode 76: total reward=91.0000, episode steps=91, trained steps=3298\n",
      "INFO:tensorflow: Episode 77: total reward=106.0000, episode steps=106, trained steps=3404\n",
      "INFO:tensorflow: Episode 78: total reward=87.0000, episode steps=87, trained steps=3491\n",
      "INFO:tensorflow: Episode 79: total reward=103.0000, episode steps=103, trained steps=3594\n",
      "INFO:tensorflow: Episode 80: total reward=108.0000, episode steps=108, trained steps=3702\n",
      "INFO:tensorflow: Episode 81: total reward=199.0000, episode steps=199, trained steps=3901\n",
      "INFO:tensorflow: Episode 82: total reward=111.0000, episode steps=111, trained steps=4012\n",
      "INFO:tensorflow: Episode 83: total reward=118.0000, episode steps=118, trained steps=4130\n",
      "INFO:tensorflow: Episode 84: total reward=170.0000, episode steps=170, trained steps=4300\n",
      "INFO:tensorflow: Episode 85: total reward=122.0000, episode steps=122, trained steps=4422\n",
      "INFO:tensorflow: Episode 86: total reward=132.0000, episode steps=132, trained steps=4554\n",
      "INFO:tensorflow: Episode 87: total reward=148.0000, episode steps=148, trained steps=4702\n",
      "INFO:tensorflow: Episode 88: total reward=178.0000, episode steps=178, trained steps=4880\n",
      "INFO:tensorflow: Episode 89: total reward=219.0000, episode steps=219, trained steps=5099\n",
      "INFO:tensorflow: Episode 90: total reward=138.0000, episode steps=138, trained steps=5237\n",
      "INFO:tensorflow: Episode 91: total reward=344.0000, episode steps=344, trained steps=5581\n",
      "INFO:tensorflow: Episode 92: total reward=131.0000, episode steps=131, trained steps=5712\n",
      "INFO:tensorflow: Episode 93: total reward=184.0000, episode steps=184, trained steps=5896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: Episode 94: total reward=115.0000, episode steps=115, trained steps=6011\n",
      "INFO:tensorflow: Episode 95: total reward=112.0000, episode steps=112, trained steps=6123\n",
      "INFO:tensorflow: Episode 96: total reward=223.0000, episode steps=223, trained steps=6346\n",
      "INFO:tensorflow: Episode 97: total reward=135.0000, episode steps=135, trained steps=6481\n",
      "INFO:tensorflow: Episode 98: total reward=115.0000, episode steps=115, trained steps=6596\n",
      "INFO:tensorflow: Episode 99: total reward=171.0000, episode steps=171, trained steps=6767\n",
      "INFO:tensorflow: Episode 100: total reward=186.0000, episode steps=186, trained steps=6953\n",
      "INFO:tensorflow: Episode 101: total reward=165.0000, episode steps=165, trained steps=7118\n",
      "INFO:tensorflow: Episode 102: total reward=81.0000, episode steps=81, trained steps=7199\n",
      "INFO:tensorflow: Episode 103: total reward=107.0000, episode steps=107, trained steps=7306\n",
      "INFO:tensorflow: Episode 104: total reward=83.0000, episode steps=83, trained steps=7389\n",
      "INFO:tensorflow: Episode 105: total reward=104.0000, episode steps=104, trained steps=7493\n",
      "INFO:tensorflow: Episode 106: total reward=107.0000, episode steps=107, trained steps=7600\n",
      "INFO:tensorflow: Episode 107: total reward=83.0000, episode steps=83, trained steps=7683\n",
      "INFO:tensorflow: Episode 108: total reward=154.0000, episode steps=154, trained steps=7837\n",
      "INFO:tensorflow: Episode 109: total reward=233.0000, episode steps=233, trained steps=8070\n",
      "INFO:tensorflow: Episode 110: total reward=177.0000, episode steps=177, trained steps=8247\n",
      "INFO:tensorflow: Episode 111: total reward=186.0000, episode steps=186, trained steps=8433\n",
      "INFO:tensorflow: Episode 112: total reward=121.0000, episode steps=121, trained steps=8554\n",
      "INFO:tensorflow: Episode 113: total reward=97.0000, episode steps=97, trained steps=8651\n",
      "INFO:tensorflow: Episode 114: total reward=91.0000, episode steps=91, trained steps=8742\n",
      "INFO:tensorflow: Episode 115: total reward=106.0000, episode steps=106, trained steps=8848\n",
      "INFO:tensorflow: Episode 116: total reward=89.0000, episode steps=89, trained steps=8937\n",
      "INFO:tensorflow: Episode 117: total reward=83.0000, episode steps=83, trained steps=9020\n",
      "INFO:tensorflow: Episode 118: total reward=102.0000, episode steps=102, trained steps=9122\n",
      "INFO:tensorflow: Episode 119: total reward=94.0000, episode steps=94, trained steps=9216\n",
      "INFO:tensorflow: Episode 120: total reward=94.0000, episode steps=94, trained steps=9310\n",
      "INFO:tensorflow: Episode 121: total reward=92.0000, episode steps=92, trained steps=9402\n",
      "INFO:tensorflow: Episode 122: total reward=123.0000, episode steps=123, trained steps=9525\n",
      "INFO:tensorflow: Episode 123: total reward=100.0000, episode steps=100, trained steps=9625\n",
      "INFO:tensorflow: Episode 124: total reward=102.0000, episode steps=102, trained steps=9727\n",
      "INFO:tensorflow: Episode 125: total reward=91.0000, episode steps=91, trained steps=9818\n",
      "INFO:tensorflow: Episode 126: total reward=95.0000, episode steps=95, trained steps=9913\n",
      "INFO:tensorflow:Agent model saved in model/model-9999, memory saved in model/agent_memory-9999.p: \n",
      "INFO:tensorflow: Episode 127: total reward=94.0000, episode steps=94, trained steps=10007\n",
      "INFO:tensorflow: Episode 128: total reward=35.0000, episode steps=35, trained steps=10042\n",
      "INFO:tensorflow: Episode 129: total reward=90.0000, episode steps=90, trained steps=10132\n",
      "INFO:tensorflow: Episode 130: total reward=103.0000, episode steps=103, trained steps=10235\n",
      "INFO:tensorflow: Episode 131: total reward=39.0000, episode steps=39, trained steps=10274\n",
      "INFO:tensorflow: Episode 132: total reward=101.0000, episode steps=101, trained steps=10375\n",
      "INFO:tensorflow: Episode 133: total reward=117.0000, episode steps=117, trained steps=10492\n",
      "INFO:tensorflow: Episode 134: total reward=109.0000, episode steps=109, trained steps=10601\n",
      "INFO:tensorflow: Episode 135: total reward=139.0000, episode steps=139, trained steps=10740\n",
      "INFO:tensorflow: Episode 136: total reward=127.0000, episode steps=127, trained steps=10867\n",
      "INFO:tensorflow: Episode 137: total reward=198.0000, episode steps=198, trained steps=11065\n",
      "INFO:tensorflow: Episode 138: total reward=190.0000, episode steps=190, trained steps=11255\n",
      "INFO:tensorflow: Episode 139: total reward=239.0000, episode steps=239, trained steps=11494\n",
      "INFO:tensorflow: Episode 140: total reward=222.0000, episode steps=222, trained steps=11716\n",
      "INFO:tensorflow: Episode 141: total reward=181.0000, episode steps=181, trained steps=11897\n",
      "INFO:tensorflow: Episode 142: total reward=113.0000, episode steps=113, trained steps=12010\n",
      "INFO:tensorflow: Episode 143: total reward=180.0000, episode steps=180, trained steps=12190\n",
      "INFO:tensorflow: Episode 144: total reward=242.0000, episode steps=242, trained steps=12432\n",
      "INFO:tensorflow: Episode 145: total reward=215.0000, episode steps=215, trained steps=12647\n",
      "INFO:tensorflow: Episode 146: total reward=189.0000, episode steps=189, trained steps=12836\n",
      "INFO:tensorflow: Episode 147: total reward=222.0000, episode steps=222, trained steps=13058\n",
      "INFO:tensorflow: Episode 148: total reward=341.0000, episode steps=341, trained steps=13399\n",
      "INFO:tensorflow: Episode 149: total reward=811.0000, episode steps=811, trained steps=14210\n",
      "INFO:tensorflow: Episode 150: total reward=354.0000, episode steps=354, trained steps=14564\n",
      "INFO:tensorflow: Episode 151: total reward=419.0000, episode steps=419, trained steps=14983\n",
      "INFO:tensorflow: Episode 152: total reward=1000.0000, episode steps=1000, trained steps=15983\n",
      "INFO:tensorflow: Episode 153: total reward=335.0000, episode steps=335, trained steps=16318\n",
      "INFO:tensorflow: Episode 154: total reward=333.0000, episode steps=333, trained steps=16651\n",
      "INFO:tensorflow: Episode 155: total reward=1000.0000, episode steps=1000, trained steps=17651\n",
      "INFO:tensorflow: Episode 156: total reward=104.0000, episode steps=104, trained steps=17755\n",
      "INFO:tensorflow: Episode 157: total reward=131.0000, episode steps=131, trained steps=17886\n",
      "INFO:tensorflow: Episode 158: total reward=109.0000, episode steps=109, trained steps=17995\n",
      "INFO:tensorflow: Episode 159: total reward=259.0000, episode steps=259, trained steps=18254\n",
      "INFO:tensorflow: Episode 160: total reward=366.0000, episode steps=366, trained steps=18620\n",
      "INFO:tensorflow: Episode 161: total reward=589.0000, episode steps=589, trained steps=19209\n",
      "INFO:tensorflow:Agent model saved in model/model-19999, memory saved in model/agent_memory-19999.p: \n",
      "INFO:tensorflow:Agent model saved in model/model-20209, memory saved in model/agent_memory-20209.p: \n"
     ]
    }
   ],
   "source": [
    "tf_agent.train(max_training_steps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Check Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: Environment '<class 'pybullet_envs.gym_pendulum_envs.InvertedPendulumBulletEnv'>' has deprecated methods. Compatibility code invoked.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import pybullet_envs\n",
    "env = gym.make(\"InvertedPendulumBulletEnv-v0\")\n",
    "env.render(mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model-19999\n"
     ]
    }
   ],
   "source": [
    "from rlagent.agents import ExperienceReplayAgent\n",
    "from rlagent.algorithms import DDPG\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.shape\n",
    "ddpg = DDPG(state_shape, action_shape, tau=0.01, actor_lr=0.0001, critic_lr=0.001,\n",
    "            action_noise=False, add_memory=False)\n",
    "tf_agent = ExperienceReplayAgent(agent=ddpg, env=env, save_steps=10000, model_dir='model')\n",
    "\n",
    "tf_agent.load_model(model_path='model/model-19999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: Episode 1: total reward=1000.0000, episode steps=1000, trained steps=20000\n",
      "INFO:tensorflow: Episode 2: total reward=1000.0000, episode steps=1000, trained steps=20000\n",
      "INFO:tensorflow: Episode 3: total reward=1000.0000, episode steps=1000, trained steps=20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-70c5e7c7e515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/myRL/rlagent/rlagent/agents/replay.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, env_render, sleep_time)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_episode_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_step_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf_agent.act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
